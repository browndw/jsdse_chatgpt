---
title: "Student Writing and Expertise in the Age of AI"
author: "Laura DeLuca, Alex Reinhart, Gordon Weinberg, Michael Laudenbach, Sydney Miller, and David West Brown"
format:
  html:
    number-sections: true
---

# Background & data

This notebook includes the code and data that were used to produce the results of our study, which was submitted to the Journal of Statistics and Data Science Education.

::: callout-note
Note that the complete raw text data is not being made available to protect the privacy of the student writers. Features extracted from the text and used for analysis are included here. Samples of text from the published and ChatGPT-generated data are included in order to demonstrate the processing pipeline.
:::

## Preparing the data

The text data were first processed using [**spacyr**](https://spacyr.quanteda.io/%5D). The resulting output was then tagged using [**psuedobibeR**](https://cmu-textstat-docs.readthedocs.io/en/latest/pseudobibeR/pseudobibeR.html). The latter is an R package that aggregates the lexicogrammatical and functonal features described by [Biber (1985)](https://books.google.mw/books?id=acTLCgAAQBAJ&printsec=frontcover#v=onepage&q&f=false) and widely used for text-type, register, and genre classification tasks.

::: callout-tip
For a primer on Biber's tags and their applications in describing language variation, see [this overview](https://www.uni-bamberg.de/fileadmin/eng-ling/fs/Chapter_21/Index.html?23DimensionsofEnglish.html) of his original study.
:::

## Load packages

```{r message=F, warning=F}
library(tidyverse)
library(gt)

# PDF output supporting special characters
library(Cairo)
```

## Load data

```{r warning=FALSE, message=FALSE}
load("data/chatgpt_data.rda")
```

## Create a composition table

```{r}
corpus_comp <- stats_freq %>%
  pivot_longer(cols = AF_chatgpt:AF_student, names_to = "Author_Type", values_to = "AF") %>%
  group_by(Author_Type) %>%
  summarize(Texts = 100,
            Tokens = sum(AF)) %>%
  mutate(Author_Type = c("ChatGPT", "Published", "Student"))
```

Used as Table 1 of the manuscript:

```{r echo=F, warning=F, message=F}
#| label: tbl-corpus
#| tbl-cap: "Composition of corpus."

corpus_comp |>
  gt() |>
  fmt_integer() |>
  cols_label(
    Author_Type = md("**Author Type**"),
    Texts = md("**Texts**"),
    Tokens = md("**Tokens**")
  ) |>
  grand_summary_rows(
    columns = c(Texts, Tokens),
    fns = list(
      Total ~ sum(.)
    ) ,
    fmt = ~ fmt_integer(.)
    )
```

# Linear discriminant analysis (LDA)

```{r message=F, warning=FALSE}
# Scale variables
biber_scaled <- stats_biber %>%
  select(-doc_id) %>%
  mutate_if(is.numeric, scale)

stats_lda <- MASS::lda(Group ~ ., biber_scaled)
```

Function for plotting...

```{r message=F, warning=FALSE}
lda_scatter <- function(lda){

  var_ex <- (lda$svd)^2/sum(lda$svd^2) * 100

  per_ex_1 <- paste0("(", round(var_ex[1], 2), "%)")
  per_ex_2 <- paste0("(", round(var_ex[2], 2), "%)")

  scores <- predict(lda) %>%
    data.frame() %>%
    select(class, x.LD1, x.LD2)

  max_x <- scores[,2] %>% abs() %>% max() %>% ceiling()+.5
  max_y <- scores[,3] %>% abs() %>% max() %>% ceiling()+.5

  p1 <- ggplot() +
    geom_hline(yintercept = 0, linewidth = .25) +
    geom_vline(xintercept = 0, linewidth = .25) +
    geom_point(data = scores, aes(x = x.LD1, y = x.LD2, fill = class),
               shape = 21, size = 1.5) +
    viridis::scale_fill_viridis(discrete = T) +
    xlab(paste0("LD1", " ", per_ex_1)) +
    ylab(paste0("LD2", " ", per_ex_2)) +
    ylim(-max_y, max_y) +
    xlim(-max_x, max_x) +
    theme_linedraw() +
    theme(panel.grid.minor.x = element_blank()) +
    theme(panel.grid.minor.y = element_blank()) +
    theme(panel.grid.major.x = element_blank()) +
    theme(panel.grid.major.y = element_blank()) +
    theme(legend.position = "none")
  return(p1)
}
```

Used as Figure 2 of the manuscript:

```{r echo=F}

p1 <- lda_scatter(stats_lda)
CairoFonts(regular = "Source Sans Pro:style=Regular")

out <- p1 +
  annotate("text", x = -9.25, y = 6, label = "↑\nNovice", size = 3.5) +
  annotate("text", x = 7.25, y = 6, label = "Human-generated →", size = 3.5) +
  annotate("text", x = 9.25, y = -6, label = "Expert\n↓", size = 3.5) +
  annotate("text", x = -7.25, y = -6, label = "← Machine-generated", size = 3.5) +
  annotate("text", x = 6.5, y = 3.75, label = "Student", size = 3.5) +
  annotate("text", x = 6, y = -3.75, label = "Published", size = 3.5) +
  annotate("text", x = -6, y = -2.5, label = "ChatGPT", size = 3.5)

ggsave("figures/lda_scatter.pdf", out, width = 5, height = 3.5,
       dev = CairoPDF)

out
```

# Multiple univariate regression

```{r message=F, warning=FALSE}
z_means <- stats_biber %>%
  select(-doc_id) %>%
  mutate_if(is.numeric, scale) %>%
  pivot_longer(!Group, names_to = "variable", values_to = "z_score") %>%
  group_by(Group, variable) %>%
  summarize(mean_z = mean(z_score)) %>%
  pivot_wider(names_from = Group, values_from = mean_z)

lm_biber <- stats_biber %>%
  select(-doc_id) %>%
  pivot_longer(!Group, names_to = "variable", values_to = "value") %>%
  group_by(variable) %>%
  arrange(.by_group = TRUE) %>%
  nest() %>%
  mutate(models = map(data, ~ lm(value ~ Group, data = .)),
         glance = map(models, broom::glance)) %>%
  unnest(glance) %>%
  select(-c(data, models)) %>%
  left_join(z_means) %>%
  select(variable, ChatGPT:Student, everything())
```

```{r message=FALSE}
ld1_tbl <- lm_biber %>%
  select(ChatGPT:r.squared, p.value) %>%
  mutate(direction = ifelse(ChatGPT > 0 & Published < 0 & Student <0, "machine", NA)) %>%
  mutate(direction = ifelse(ChatGPT < 0 & Published > 0 & Student > 0, "human", direction)) %>%
  filter(!is.na(direction)) %>%
  filter(r.squared > 0.1) %>%
  arrange(direction, -r.squared)

ld2_tbl <- lm_biber %>%
  select(ChatGPT:r.squared, p.value) %>%
  mutate(direction = ifelse(Published > 0 & ChatGPT < 0 & Student <0, "expert", NA)) %>%
  mutate(direction = ifelse(Student > 0 & ChatGPT < 0 & Published < 0, "novice", direction)) %>%
  filter(!is.na(direction)) %>%
  filter(r.squared > 0.1) %>%
  arrange(desc(direction), -r.squared)
```

Used as Table 2 of the manuscript:

```{r echo=F, warning=F, message=F}

ld1_tbl |>
  mutate(direction = paste0("Features indicating ", direction, "-generated writing")) |>
  mutate(variable = str_remove(variable, "f_\\d+_")) |>
  mutate(variable = str_replace_all(variable, "_", " ")) |>
  mutate(p.value = p.value * 67) |> # Bonferroni
  gt(groupname_col = 'direction') |>
  cols_label(
    variable = md(""),
    ChatGPT = md("**ChatGPT**<br>$n = 100$"),
    Published = md("**Published**<br>$n = 100$"),
    Student = md("**Student**<br>$n = 100$"),
    r.squared = md("***R*^2^**"),
    p.value = md("***p*-value**")
  ) |>
  fmt_number(
    columns = !p.value,
    decimals = 2
  )  |>
  fmt(
    p.value,
    fns = scales::label_pvalue()
  ) |>
  data_color(
    columns = c(ChatGPT:Student),
    colors = scales::col_numeric(
      palette = c(
        "#FF6666", "white", "#336699"),
      domain = c(pmin(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student),
                 0,
                 pmax(ld1_tbl$ChatGPT, ld1_tbl$Published, ld1_tbl$Student)))
  ) |>
  tab_style(
    style = list(
      cell_text(style = "italic",
                align = "right")
      ),
    locations = cells_body(
      columns = variable,
    )
  )
```

And Table 4:

```{r echo=F, warning=F, message=F}
ld2_tbl |>
  mutate(direction = paste0("Features indicating ", direction, " writing")) |>
  mutate(variable = str_remove(variable, "f_\\d+_")) |>
  mutate(variable = str_replace_all(variable, "_", " ")) |>
  gt(groupname_col = 'direction') |>
  cols_label(
    variable = md(""),
    ChatGPT = md("**ChatGPT**<br>$n = 100$"),
    Published = md("**Published**<br>$n = 100$"),
    Student = md("**Student**<br>$n = 100$"),
    r.squared = md("***R*^2^**"),
    p.value = md("***p*-value**")
  ) |>
  fmt_number(
    columns = !p.value,
    decimals = 2
  )  |>
  fmt(
    p.value,
    fns = scales::label_pvalue()
  ) |>
  data_color(
    columns = c(ChatGPT:Student),
    colors = scales::col_numeric(
      palette = c(
        "#FF6666", "white", "#336699"),
      domain = c(-1, 0, 1))
  ) |>
  tab_style(
    style = list(
      cell_text(style = "italic",
                align = "right")
      ),
    locations = cells_body(
      columns = variable,
    )
  )
```

# Modal verbs

## Table of modal verb frequencies

Table 3 of the manuscript:

```{r echo=F, warning=F, message=F}

stats_freq |>
  filter(tag == "md") |>
  select(-tag) |>
  mutate(modal_type = ifelse(str_detect(token, "will|would|'ll"), "Prediction", NA)) |>
  mutate(modal_type = ifelse(str_detect(token, "can|may|could|might"), "Possiblity", modal_type)) |>
  mutate(modal_type = ifelse(is.na(modal_type), "Necessity", modal_type)) |>
  gt(groupname_col = 'modal_type') |>
  cols_label(
    token = md("Modal verb"),
    AF_chatgpt = md("ChatGPT"),
    AF_published = md("Published"),
    AF_student = md("Student"),
    RF_chatgpt = md("ChatGPT"),
    RF_published = md("Published"),
    RF_student = md("Student"),
  ) |>
  tab_spanner(
    label = "Absolute Frequency",
    columns = c(AF_chatgpt, AF_published, AF_student)
  ) |>
  tab_spanner(
    label = md("Relative Frequency (per 10^5^ words)"),
    columns = c(RF_chatgpt, RF_published, RF_student)
  ) |>
  fmt_number(
    columns = c(RF_chatgpt, RF_published, RF_student),
    decimals = 2
  ) |>
  tab_style(
    style = list(
      cell_text(style = "italic",
                align = "right")
      ),
    locations = cells_body(
      columns = token,
    )
  )
```

## Modal verb position in full report

Read in the report and format for plotting.

```{r warning=F, message=F}

# read in the full report
doc <- readtext::readtext("data/gpt-full-report.txt")

# tokenize the document
doc_tks <- doc %>%
  quanteda::corpus() %>%
  quanteda::tokens() %>%
  quanteda::as.list() %>%
  data.frame() %>%
  rename(token = "gpt.full.report.txt") %>%
  mutate(section = ifelse(str_detect(token, "^Abstract$|^Introduction$|^Methods$|^Results$|^Discussion$"), token, NA))

# add column for modal verb types
doc_tks <- doc_tks %>%
  mutate(modal_type = ifelse(str_detect(token, "^will$|^would$|^'ll$"), "Prediction", NA)) %>%
  mutate(modal_type = ifelse(str_detect(token, "^can$|^may$|^could$|^might$"), "Possiblity", modal_type)) %>%
  mutate(modal_type = ifelse(str_detect(token, "^should$|^must$"), "Necessity", modal_type))

# add index for token position
doc_tks <- doc_tks %>%
  rownames_to_column("idx") %>%
  mutate(idx = as.numeric(idx)) %>%
  mutate(idx = idx/nrow(doc_tks))

# format data for plotting
doc_tks <- doc_tks %>%
  filter(!is.na(modal_type) | !is.na(section) ) %>%
  fill(section) %>%
  group_by(token) %>%
  mutate(total = n()) %>%
  ungroup() %>%
  mutate(label = ifelse(!is.na(modal_type), paste0("italic(",token, ")~~(n == ", total, ")"), NA))

```

## Plot

Figure 5 of the manuscript:

```{r warning=F, message=F}

p1 <- ggplot(data = filter(doc_tks, !is.na(modal_type))) +
  geom_segment(aes(x = idx, y = 0, xend = idx, yend = 1), color = "black") +
  annotate('rect', xmin=0, xmax=min(doc_tks$idx[doc_tks$section == "Introduction"]), ymin=0, ymax=1, alpha=.2, fill='red') +
  annotate('rect', xmin=min(doc_tks$idx[doc_tks$section == "Methods"]), xmax=min(doc_tks$idx[doc_tks$section == "Results"]), ymin=0, ymax=1, alpha=.2, fill='red') +
  annotate('rect', xmin=min(doc_tks$idx[doc_tks$section == "Discussion"]), xmax=1, ymin=0, ymax=1, alpha=.2, fill='red') +
  theme_classic() +
  theme(
    axis.line = element_blank(),
    panel.background = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    plot.background = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
    strip.text.y = element_text(angle = 0),
    axis.title.x=element_text(size=9),
  ) +
  labs(x = "Relative token index", y = "") +
  facet_wrap(vars(label), nrow = 4, labeller = label_parsed) +
  theme(strip.text = element_text(
    size = 12))

dat_text <- data.frame(
  sect_label = c("A", "I", "M", "R", "D"),
  label   = c("italic(would)~~(n == 1)", "italic(would)~~(n == 1)", "italic(would)~~(n == 1)", "italic(would)~~(n == 1)", "italic(would)~~(n == 1)"),
  x     = c(
    .025,
    min(doc_tks$idx[doc_tks$section == "Introduction"]) + .025,
    min(doc_tks$idx[doc_tks$section == "Methods"] + .025),
    min(doc_tks$idx[doc_tks$section == "Results"] + .025),
    min(doc_tks$idx[doc_tks$section == "Discussion"] + .035)
  ),
  y     = c(0.15, 0.15, 0.15, 0.15, 0.15)
)

out <- p1 + geom_text(
  data    = dat_text,
  mapping = aes(x = x, y = y, label = sect_label), fontface='bold'
)

out
```

# Noun phrases

Lengths of noun phrases.

```{r warning=F, message=F}
df_plot <- stats_nps  %>%
  filter(!is.na(n_pre) | !is.na(n_post)) %>%
  group_by(author_type) %>%
  summarize(np_len = mean(np_len),
            pre_root = mean(n_pre),
            post_root = mean(n_post)) %>%
  mutate(author_type = c("ChatGPT", "Published", "Student"))

df_plot <- within(df_plot, author_type <- factor(author_type, levels = c('Student', 'ChatGPT', 'Published')))

```

## Build plot

```{r}
text_center <- grid::textGrob("Root noun", gp= grid::gpar(fontsize=10, fontface="bold"))
text_left <- grid::textGrob("Pre-nominal", gp= grid::gpar(fontsize=10, fontface="bold"))
text_right <- grid::textGrob("Post-nominal", gp= grid::gpar(fontsize=10, fontface="bold"))

g.mid <- ggplot(df_plot, aes(x=1, y=author_type)) +
  geom_text(aes(label = paste0("- ", author_type, " -")), lineheight = 1) +
  ggtitle("") +
  ylab(NULL) +
  annotation_custom(text_center, xmin=1, xmax=1, ymin=-0.5, ymax=1.5) +
  coord_cartesian(clip = "off") +
  theme(axis.title=element_blank(),
        panel.grid=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.background=element_blank(),
        axis.text.x=element_text(color=NA),
        axis.ticks.x=element_line(color=NA),
        plot.margin = unit(c(1, -1, 1.25, -1), "lines"))

g1 <- ggplot(data = df_plot, aes(x = author_type, y = pre_root, fill = author_type)) +
  geom_col(width = 0.5) + ggtitle("") +
  scale_fill_manual(values = c(
    "Published" = viridis::viridis(3)[2],
    "ChatGPT"   = viridis::viridis(3)[1],
    "Student"   = viridis::viridis(3)[3])
    ) +
  geom_text(
    aes(y = .55, label = paste0("← ", round(pre_root, 2), " words")),
    nudge_x = .5
  ) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        legend.position = "none",
        axis.ticks.x = element_blank(),
        panel.background = element_blank(),
        plot.margin = unit(c(1, -1, 2, 0), "lines")) +
  annotation_custom(text_left, xmin=-0.5, xmax=1.5, ymin=-0.5, ymax=-0.5) +
  scale_y_reverse() +
  coord_flip()

g2 <- ggplot(data = df_plot, aes(x = author_type, y = post_root, fill = author_type)) +
  xlab(NULL) +
  geom_col(width = 0.5) + ggtitle("") +
  scale_fill_manual(values = c(
    "Published" = viridis::viridis(3)[2],
    "ChatGPT"   = viridis::viridis(3)[1],
    "Student"   = viridis::viridis(3)[3])
  ) +
  geom_text(
    aes(y = .5, label = paste0(round(post_root, 2), " words →")),
    nudge_x = .5
  ) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = "none",
        panel.background = element_blank(),
        plot.margin = unit(c(1, 0, 2, -1), "lines")) +
  annotation_custom(text_right, xmin=-0.5, xmax=1.5, ymin=0.5, ymax=0.5) +
  coord_flip()


gg1 <- ggplot_gtable(ggplot_build(g1))
gg2 <- ggplot_gtable(ggplot_build(g2))
gg.mid <- ggplot_gtable(ggplot_build(g.mid))
```

## Plot

Figure 4 of the manuscript:

```{r echo=F, warning=F, message=F}
#| fig-height: 3

gridExtra::grid.arrange(gg1, gg.mid, gg2, ncol=3, widths=c(2.2/10, 1.7/10, 6.1/10))
```
